{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_mnist(task_num=2):\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "    mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    \n",
    "    if task_num == 1:\n",
    "        train_indices = [i for i, target in enumerate(mnist_train.targets) if target in [0, 1, 2, 3, 4]]\n",
    "        test_indices = [i for i, target in enumerate(mnist_test.targets) if target in [0, 1, 2, 3, 4]]\n",
    "    elif task_num == 2:\n",
    "        train_indices = [i for i, target in enumerate(mnist_train.targets) if target in [5, 6, 7, 8, 9]]\n",
    "        test_indices = [i for i, target in enumerate(mnist_test.targets) if target in [5, 6, 7, 8, 9]]\n",
    "\n",
    "    train_dataset = Subset(mnist_train, train_indices)\n",
    "    test_dataset = Subset(mnist_test, test_indices)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EWC:\n",
    "    def __init__(self, model, dataset):\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.params = {n: p.clone().detach() for n, p in self.model.named_parameters() if p.requires_grad}\n",
    "        self._precision_matrices = self._calculate_fisher()\n",
    "\n",
    "    def _calculate_fisher(self):\n",
    "        precision_matrices = {n: torch.zeros(p.size()) for n, p in self.model.named_parameters() if p.requires_grad}\n",
    "        self.model.eval()\n",
    "        \n",
    "        for data in self.dataset:\n",
    "            inputs, labels = data\n",
    "            self.model.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            for n, p in self.model.named_parameters():\n",
    "                if p.requires_grad:\n",
    "                    precision_matrices[n].data += p.grad.data ** 2 / len(self.dataset)\n",
    "        \n",
    "        return precision_matrices\n",
    "\n",
    "    def penalty(self, model):\n",
    "        loss = 0\n",
    "        for n, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                _loss = self._precision_matrices[n] * (p - self.params[n]) ** 2\n",
    "                loss += _loss.sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, dataloader, ewc=None, ewc_lambda=0):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        if ewc:\n",
    "            loss += ewc_lambda * ewc.penalty(model)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy on Task 1: 95.97%\n",
      "Test Accuracy on Task 2: 91.73%\n",
      "Test Accuracy on Task 1 (revisited): 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = SimpleNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train on Task 1\n",
    "train_loader_1, test_loader_1 = get_split_mnist(task_num=1)\n",
    "train_model(model, optimizer, criterion, train_loader_1)\n",
    "\n",
    "# Evaluate on Task 1\n",
    "accuracy_1 = evaluate_model(model, test_loader_1)\n",
    "print(f'Test Accuracy on Task 1: {accuracy_1 * 100:.2f}%')\n",
    "\n",
    "# Apply EWC for Task 2\n",
    "ewc = EWC(model, train_loader_1)\n",
    "\n",
    "# Train on Task 2\n",
    "train_loader_2, test_loader_2 = get_split_mnist(task_num=2)\n",
    "ewc_lambda = 0.4\n",
    "train_model(model, optimizer, criterion, train_loader_2, ewc, ewc_lambda)\n",
    "\n",
    "# Evaluate on Task 2\n",
    "accuracy_2 = evaluate_model(model, test_loader_2)\n",
    "print(f'Test Accuracy on Task 2: {accuracy_2 * 100:.2f}%')\n",
    "\n",
    "# Evaluate on Task 1 again\n",
    "accuracy_1_revisited = evaluate_model(model, test_loader_1)\n",
    "print(f'Test Accuracy on Task 1 (revisited): {accuracy_1_revisited * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
