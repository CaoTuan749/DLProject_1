{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n",
      "\n",
      "[INFO] Training on Task 0 with classes: [0, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 18:18:47,318] A new study created in memory with name: no-name-245418dd-015d-444a-8402-87133d63f77f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Creating ResNet50 with lr=3.088271973902033e-05, weight_decay=0.007074911583202907\n",
      "[DEBUG] Creating ResNet50 with lr=3.088271973902033e-05, weight_decay=0.007074911583202907\n",
      "[DEBUG] Creating ResNet50 with lr=3.088271973902033e-05, weight_decay=0.007074911583202907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 18:27:31,393] Trial 0 finished with value: 0.026745706890887094 and parameters: {'lr': 3.088271973902033e-05, 'weight_decay': 0.007074911583202907}. Best is trial 0 with value: 0.026745706890887094.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Creating ResNet50 with lr=0.00013112763125612312, weight_decay=0.004022552330099805\n",
      "[DEBUG] Creating ResNet50 with lr=0.00013112763125612312, weight_decay=0.004022552330099805\n",
      "[DEBUG] Creating ResNet50 with lr=0.00013112763125612312, weight_decay=0.004022552330099805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 18:37:21,883] Trial 1 finished with value: 0.0127835709450116 and parameters: {'lr': 0.00013112763125612312, 'weight_decay': 0.004022552330099805}. Best is trial 1 with value: 0.0127835709450116.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Best hyperparameters: lr=0.00013112763125612312, weight_decay=0.004022552330099805\n",
      "[DEBUG] Creating ResNet50 with lr=0.00013112763125612312, weight_decay=0.004022552330099805\n",
      "Epoch 0: Train Loss=0.0751, Val Loss=0.0979\n",
      "Epoch 1: Train Loss=0.0063, Val Loss=0.1629\n",
      "Epoch 2: Train Loss=0.0141, Val Loss=0.1795\n",
      "Epoch 3: Train Loss=0.0146, Val Loss=0.0950\n",
      "Epoch 4: Train Loss=0.0137, Val Loss=0.2481\n",
      "Epoch 5: Train Loss=0.0043, Val Loss=0.1009\n",
      "Epoch 6: Train Loss=0.0053, Val Loss=0.1116\n",
      "Epoch 7: Train Loss=0.0208, Val Loss=0.1048\n",
      "Epoch 8: Train Loss=0.0208, Val Loss=0.1500\n",
      "Epoch 9: Train Loss=0.0067, Val Loss=0.3039\n",
      "[INFO] Model checkpoint saved to model_checkpoints\\baseline\\default\\task0\\model_default.pth\n",
      "[Evaluation] Task 0 test set accuracy: 0.8978\n",
      "[Evaluation] Classification Report for Task 0:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.894     0.999     0.943       832\n",
      "           1      0.979     0.322     0.485       146\n",
      "\n",
      "    accuracy                          0.898       978\n",
      "   macro avg      0.936     0.660     0.714       978\n",
      "weighted avg      0.906     0.898     0.875       978\n",
      "\n",
      "[Evaluation] Cumulative test set accuracy (tasks 0 to 0): 0.8978\n",
      "[Evaluation] Cumulative Classification Report (tasks 0 to 0):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.894     0.999     0.943       832\n",
      "           1      0.979     0.322     0.485       146\n",
      "\n",
      "    accuracy                          0.898       978\n",
      "   macro avg      0.936     0.660     0.714       978\n",
      "weighted avg      0.906     0.898     0.875       978\n",
      "\n",
      "\n",
      "[INFO] Training on Task 1 with classes: [2, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 18:43:52,959] A new study created in memory with name: no-name-057d7e7b-8012-4358-8111-44280efc6e6f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Creating ResNet50 with lr=0.0002955238435626053, weight_decay=6.107669733918394e-05\n",
      "[DEBUG] Creating ResNet50 with lr=0.0002955238435626053, weight_decay=6.107669733918394e-05\n",
      "[DEBUG] Creating ResNet50 with lr=0.0002955238435626053, weight_decay=6.107669733918394e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 19:15:45,210] Trial 0 finished with value: 0.02761032228502078 and parameters: {'lr': 0.0002955238435626053, 'weight_decay': 6.107669733918394e-05}. Best is trial 0 with value: 0.02761032228502078.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Creating ResNet50 with lr=2.399676281099289e-05, weight_decay=2.5787867718315216e-05\n",
      "[DEBUG] Creating ResNet50 with lr=2.399676281099289e-05, weight_decay=2.5787867718315216e-05\n",
      "[DEBUG] Creating ResNet50 with lr=2.399676281099289e-05, weight_decay=2.5787867718315216e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-15 19:52:21,201] Trial 1 finished with value: 0.030530131620692152 and parameters: {'lr': 2.399676281099289e-05, 'weight_decay': 2.5787867718315216e-05}. Best is trial 0 with value: 0.02761032228502078.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Best hyperparameters: lr=0.0002955238435626053, weight_decay=6.107669733918394e-05\n",
      "Epoch 0: Train Loss=1.1696, Val Loss=1.1882\n",
      "Epoch 1: Train Loss=0.3609, Val Loss=0.7372\n",
      "Epoch 2: Train Loss=0.2489, Val Loss=0.5412\n",
      "Epoch 3: Train Loss=0.1992, Val Loss=0.5082\n",
      "Epoch 4: Train Loss=0.1628, Val Loss=0.5180\n",
      "Epoch 5: Train Loss=0.1294, Val Loss=0.5003\n",
      "Epoch 6: Train Loss=0.1103, Val Loss=0.4122\n",
      "Epoch 7: Train Loss=0.0981, Val Loss=0.4043\n",
      "Epoch 8: Train Loss=0.0906, Val Loss=0.4413\n",
      "Epoch 9: Train Loss=0.0850, Val Loss=0.3705\n",
      "[INFO] Model checkpoint saved to model_checkpoints\\baseline\\default\\task1\\model_default.pth\n",
      "[Evaluation] Task 1 test set accuracy: 0.8625\n",
      "[Evaluation] Classification Report for Task 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.873     0.944     0.907      2772\n",
      "           1      0.827     0.663     0.736      1126\n",
      "\n",
      "    accuracy                          0.862      3898\n",
      "   macro avg      0.850     0.803     0.821      3898\n",
      "weighted avg      0.860     0.862     0.858      3898\n",
      "\n",
      "[Evaluation] Cumulative test set accuracy (tasks 0 to 1): 0.1704\n",
      "[Evaluation] Cumulative Classification Report (tasks 0 to 1):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.208     0.992     0.344       832\n",
      "           1      0.007     0.041     0.011       146\n",
      "           2      0.000     0.000     0.000      2772\n",
      "           3      0.000     0.000     0.000      1126\n",
      "\n",
      "    accuracy                          0.170      4876\n",
      "   macro avg      0.054     0.258     0.089      4876\n",
      "weighted avg      0.036     0.170     0.059      4876\n",
      "\n",
      "[INFO] Continual learning training completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "import torch\n",
    "from utils import (\n",
    "    load_config,\n",
    "    set_seed\n",
    ")\n",
    "from train_continual import continual_training_pipeline\n",
    "\n",
    "\n",
    "def main():\n",
    "    config = load_config(\"config.yaml\")\n",
    "    if config[\"experiment\"].get(\"reproducibility\", False):\n",
    "        set_seed(config[\"experiment\"].get(\"seed\", 42))\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "    if config[\"model\"][\"type\"] == \"resnet50\":\n",
    "        from Models.Wafer_resnet_model import create_resnet_model\n",
    "        model_factory = create_resnet_model\n",
    "    elif config[\"model\"][\"type\"] == \"simplenn\":\n",
    "        from Models.Wafer_simple_model import create_simple_model\n",
    "        model_factory = create_simple_model\n",
    "\n",
    "    continual_training_pipeline(config, model_factory, device)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration:\n",
      "{'experiment': {'reproducibility': True, 'seed': 42, 'num_trials': 2, 'num_epochs': 10, 'final_epochs': 10, 'save_model': True, 'final_model_filename': 'final_model.pth', 'checkpoint_base_dir': 'model_checkpoints', 'tensorboard_log_dir': 'Logs', 'continual_learning': True, 'continual_method': 'baseline', 'ewc_lambda': 100.0, 'task_list': [[0, 1], [2, 3]], 'suggest': {'lr': {'low': '1e-5', 'high': '1e-3', 'log': True}, 'weight_decay': {'low': '1e-6', 'high': '1e-2', 'log': True}}}, 'logging': {'base_log_dir': './Logs'}, 'model': {'type': 'resnet50', 'lr': '1e-4', 'weight_decay': '1e-4'}, 'dataset': {'path': 'D:/Waffer Data/WM811K.pkl'}}\n",
      "[INFO] Reproducibility enabled. Seed set to 42\n",
      "[INFO] Using device: cuda\n",
      "[INFO] Number of classes: 8\n",
      "[INFO] Classes: ['Center' 'Donut' 'Edge-Loc' 'Edge-Ring' 'Loc' 'Near-full' 'Random'\n",
      " 'Scratch']\n",
      "[INFO] Creating Optuna study 'resnet_wafer_1742165514' with DB file: sqlite:///resnet_wafer_v2.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-16 23:51:54,983] A new study created in RDB with name: resnet_wafer_1742165514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting study.optimize with 2 trials...\n",
      "\n",
      "\n",
      "[OPTUNA] Starting Trial #0 with lr=0.000059, weight_decay=0.000518\n",
      "[OPTUNA] Trial #0: Starting fold 1/5\n",
      "[DEBUG] Creating ResNet50 with lr=5.850406800780064e-05, weight_decay=0.0005180819007082417\n",
      "[INFO] >>> Starting fold 1 training (Trial #0) for 10 epochs...\n",
      "[Fold 1 Epoch 1/10] Train Loss: 0.2988, Val Loss: 0.1079\n",
      "[Fold 1 Epoch 2/10] Train Loss: 0.0725, Val Loss: 0.0958\n",
      "[Fold 1 Epoch 3/10] Train Loss: 0.0366, Val Loss: 0.1276\n",
      "[Fold 1 Epoch 4/10] Train Loss: 0.0255, Val Loss: 0.1117\n",
      "[INFO] Early stopping on fold 1 at epoch 4\n",
      "[INFO] <<< Finished fold 1, best val loss = 0.0958\n",
      "\n",
      "[OPTUNA] Trial #0: Starting fold 2/5\n",
      "[DEBUG] Creating ResNet50 with lr=5.850406800780064e-05, weight_decay=0.0005180819007082417\n",
      "[INFO] >>> Starting fold 2 training (Trial #0) for 10 epochs...\n",
      "[Fold 2 Epoch 1/10] Train Loss: 0.3066, Val Loss: 0.1060\n",
      "[Fold 2 Epoch 2/10] Train Loss: 0.0756, Val Loss: 0.1031\n",
      "[Fold 2 Epoch 3/10] Train Loss: 0.0330, Val Loss: 0.1047\n",
      "[Fold 2 Epoch 4/10] Train Loss: 0.0215, Val Loss: 0.0826\n",
      "[Fold 2 Epoch 5/10] Train Loss: 0.0182, Val Loss: 0.0869\n",
      "[Fold 2 Epoch 6/10] Train Loss: 0.0237, Val Loss: 0.0768\n",
      "[Fold 2 Epoch 7/10] Train Loss: 0.0143, Val Loss: 0.0781\n",
      "[Fold 2 Epoch 8/10] Train Loss: 0.0180, Val Loss: 0.0840\n",
      "[INFO] Early stopping on fold 2 at epoch 8\n",
      "[INFO] <<< Finished fold 2, best val loss = 0.0768\n",
      "\n",
      "[OPTUNA] Trial #0: Starting fold 3/5\n",
      "[DEBUG] Creating ResNet50 with lr=5.850406800780064e-05, weight_decay=0.0005180819007082417\n",
      "[INFO] >>> Starting fold 3 training (Trial #0) for 10 epochs...\n",
      "[Fold 3 Epoch 1/10] Train Loss: 0.2996, Val Loss: 0.0971\n",
      "[Fold 3 Epoch 2/10] Train Loss: 0.0752, Val Loss: 0.0894\n",
      "[Fold 3 Epoch 3/10] Train Loss: 0.0315, Val Loss: 0.1260\n",
      "[Fold 3 Epoch 4/10] Train Loss: 0.0268, Val Loss: 0.1048\n",
      "[INFO] Early stopping on fold 3 at epoch 4\n",
      "[INFO] <<< Finished fold 3, best val loss = 0.0894\n",
      "\n",
      "[OPTUNA] Trial #0: Starting fold 4/5\n",
      "[DEBUG] Creating ResNet50 with lr=5.850406800780064e-05, weight_decay=0.0005180819007082417\n",
      "[INFO] >>> Starting fold 4 training (Trial #0) for 10 epochs...\n",
      "[Fold 4 Epoch 1/10] Train Loss: 0.3019, Val Loss: 0.1369\n",
      "[Fold 4 Epoch 2/10] Train Loss: 0.0718, Val Loss: 0.1090\n",
      "[Fold 4 Epoch 3/10] Train Loss: 0.0387, Val Loss: 0.1351\n",
      "[Fold 4 Epoch 4/10] Train Loss: 0.0224, Val Loss: 0.0987\n",
      "[Fold 4 Epoch 5/10] Train Loss: 0.0196, Val Loss: 0.1294\n",
      "[Fold 4 Epoch 6/10] Train Loss: 0.0182, Val Loss: 0.1317\n",
      "[INFO] Early stopping on fold 4 at epoch 6\n",
      "[INFO] <<< Finished fold 4, best val loss = 0.0987\n",
      "\n",
      "[OPTUNA] Trial #0: Starting fold 5/5\n",
      "[DEBUG] Creating ResNet50 with lr=5.850406800780064e-05, weight_decay=0.0005180819007082417\n",
      "[INFO] >>> Starting fold 5 training (Trial #0) for 10 epochs...\n",
      "[Fold 5 Epoch 1/10] Train Loss: 0.3119, Val Loss: 0.1064\n",
      "[Fold 5 Epoch 2/10] Train Loss: 0.0774, Val Loss: 0.1361\n",
      "[Fold 5 Epoch 3/10] Train Loss: 0.0435, Val Loss: 0.0914\n",
      "[Fold 5 Epoch 4/10] Train Loss: 0.0265, Val Loss: 0.0899\n",
      "[Fold 5 Epoch 5/10] Train Loss: 0.0215, Val Loss: 0.1023\n",
      "[Fold 5 Epoch 6/10] Train Loss: 0.0200, Val Loss: 0.1186\n",
      "[INFO] Early stopping on fold 5 at epoch 6\n",
      "[INFO] <<< Finished fold 5, best val loss = 0.0899\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-17 00:29:51,585] Trial 0 finished with value: 0.9098772154754051 and parameters: {'lr': 5.850406800780064e-05, 'weight_decay': 0.0005180819007082417}. Best is trial 0 with value: 0.9098772154754051.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OPTUNA] Trial #0 done. Fold Accuracies: [0.904204009510847, 0.9232247300491768, 0.9105543547962692, 0.9012749006433296, 0.9101280823774037]. Avg Acc=0.9099\n",
      "\n",
      "[OPTUNA] Starting Trial #1 with lr=0.000317, weight_decay=0.000003\n",
      "[OPTUNA] Trial #1: Starting fold 1/5\n",
      "[DEBUG] Creating ResNet50 with lr=0.00031673411752139316, weight_decay=2.7922343178785083e-06\n",
      "[INFO] >>> Starting fold 1 training (Trial #1) for 10 epochs...\n",
      "[Fold 1 Epoch 1/10] Train Loss: 0.2771, Val Loss: 0.2996\n",
      "[Fold 1 Epoch 2/10] Train Loss: 0.1352, Val Loss: 0.1462\n",
      "[Fold 1 Epoch 3/10] Train Loss: 0.0919, Val Loss: 0.1382\n",
      "[Fold 1 Epoch 4/10] Train Loss: 0.0725, Val Loss: 0.1397\n",
      "[Fold 1 Epoch 5/10] Train Loss: 0.0600, Val Loss: 0.1775\n",
      "[INFO] Early stopping on fold 1 at epoch 5\n",
      "[INFO] <<< Finished fold 1, best val loss = 0.1382\n",
      "\n",
      "[OPTUNA] Trial #1: Starting fold 2/5\n",
      "[DEBUG] Creating ResNet50 with lr=0.00031673411752139316, weight_decay=2.7922343178785083e-06\n",
      "[INFO] >>> Starting fold 2 training (Trial #1) for 10 epochs...\n",
      "[Fold 2 Epoch 1/10] Train Loss: 0.2690, Val Loss: 0.1613\n",
      "[Fold 2 Epoch 2/10] Train Loss: 0.1197, Val Loss: 0.1709\n",
      "[Fold 2 Epoch 3/10] Train Loss: 0.0815, Val Loss: 0.1636\n",
      "[INFO] Early stopping on fold 2 at epoch 3\n",
      "[INFO] <<< Finished fold 2, best val loss = 0.1613\n",
      "\n",
      "[OPTUNA] Trial #1: Starting fold 3/5\n",
      "[DEBUG] Creating ResNet50 with lr=0.00031673411752139316, weight_decay=2.7922343178785083e-06\n",
      "[INFO] >>> Starting fold 3 training (Trial #1) for 10 epochs...\n",
      "[Fold 3 Epoch 1/10] Train Loss: 0.2674, Val Loss: 0.1948\n",
      "[Fold 3 Epoch 2/10] Train Loss: 0.1296, Val Loss: 0.1690\n",
      "[Fold 3 Epoch 3/10] Train Loss: 0.0936, Val Loss: 0.1406\n",
      "[Fold 3 Epoch 4/10] Train Loss: 0.0692, Val Loss: 0.1325\n",
      "[Fold 3 Epoch 5/10] Train Loss: 0.0626, Val Loss: 0.1603\n",
      "[Fold 3 Epoch 6/10] Train Loss: 0.0514, Val Loss: 0.1664\n",
      "[INFO] Early stopping on fold 3 at epoch 6\n",
      "[INFO] <<< Finished fold 3, best val loss = 0.1325\n",
      "\n",
      "[OPTUNA] Trial #1: Starting fold 4/5\n",
      "[DEBUG] Creating ResNet50 with lr=0.00031673411752139316, weight_decay=2.7922343178785083e-06\n",
      "[INFO] >>> Starting fold 4 training (Trial #1) for 10 epochs...\n",
      "[Fold 4 Epoch 1/10] Train Loss: 0.2654, Val Loss: 0.1626\n",
      "[Fold 4 Epoch 2/10] Train Loss: 0.1344, Val Loss: 0.1320\n",
      "[Fold 4 Epoch 3/10] Train Loss: 0.0840, Val Loss: 0.1203\n",
      "[Fold 4 Epoch 4/10] Train Loss: 0.0650, Val Loss: 0.1803\n",
      "[Fold 4 Epoch 5/10] Train Loss: 0.0571, Val Loss: 0.1197\n",
      "[Fold 4 Epoch 6/10] Train Loss: 0.0388, Val Loss: 0.1627\n",
      "[Fold 4 Epoch 7/10] Train Loss: 0.0361, Val Loss: 0.1457\n",
      "[INFO] Early stopping on fold 4 at epoch 7\n",
      "[INFO] <<< Finished fold 4, best val loss = 0.1197\n",
      "\n",
      "[OPTUNA] Trial #1: Starting fold 5/5\n",
      "[DEBUG] Creating ResNet50 with lr=0.00031673411752139316, weight_decay=2.7922343178785083e-06\n",
      "[INFO] >>> Starting fold 5 training (Trial #1) for 10 epochs...\n",
      "[Fold 5 Epoch 1/10] Train Loss: 0.2872, Val Loss: 0.1859\n",
      "[Fold 5 Epoch 2/10] Train Loss: 0.1355, Val Loss: 0.1397\n",
      "[Fold 5 Epoch 3/10] Train Loss: 0.0923, Val Loss: 0.1621\n",
      "[Fold 5 Epoch 4/10] Train Loss: 0.0705, Val Loss: 0.1087\n",
      "[Fold 5 Epoch 5/10] Train Loss: 0.0693, Val Loss: 0.1369\n",
      "[Fold 5 Epoch 6/10] Train Loss: 0.0452, Val Loss: 0.2517\n",
      "[INFO] Early stopping on fold 5 at epoch 6\n",
      "[INFO] <<< Finished fold 5, best val loss = 0.1087\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-17 01:08:43,639] Trial 1 finished with value: 0.8679034621403648 and parameters: {'lr': 0.00031673411752139316, 'weight_decay': 2.7922343178785083e-06}. Best is trial 0 with value: 0.9098772154754051.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OPTUNA] Trial #1 done. Fold Accuracies: [0.8617526181795776, 0.8386558398050121, 0.8674732660866928, 0.880325255598856, 0.8913103310316858]. Avg Acc=0.8679\n",
      "[OPTUNA] Best trial found:\n",
      "  Trial number: 0\n",
      "  Avg k-fold acc: 0.9099\n",
      "    lr: 5.850406800780064e-05\n",
      "    weight_decay: 0.0005180819007082417\n",
      "\n",
      "[INFO] Retraining final model on entire training set using best hyperparameters...\n",
      "[DEBUG] Creating ResNet50 with lr=5.850406800780064e-05, weight_decay=0.0005180819007082417\n",
      "[Final Train] Epoch 1/10 - Loss: 0.2684, Acc: 0.9092\n",
      "[Final Train] Epoch 2/10 - Loss: 0.0705, Acc: 0.9774\n",
      "[Final Train] Epoch 3/10 - Loss: 0.0350, Acc: 0.9886\n",
      "[Final Train] Epoch 4/10 - Loss: 0.0285, Acc: 0.9912\n",
      "[Final Train] Epoch 5/10 - Loss: 0.0222, Acc: 0.9934\n",
      "[Final Train] Epoch 6/10 - Loss: 0.0186, Acc: 0.9942\n",
      "[Final Train] Epoch 7/10 - Loss: 0.0114, Acc: 0.9969\n",
      "[Final Train] Epoch 8/10 - Loss: 0.0119, Acc: 0.9970\n",
      "[Final Train] Epoch 9/10 - Loss: 0.0113, Acc: 0.9965\n",
      "[Final Train] Epoch 10/10 - Loss: 0.0184, Acc: 0.9949\n",
      "\n",
      "[INFO] Evaluating final model on test set...\n",
      "[RESULT] Final Test Accuracy: 0.7859\n",
      "\n",
      "[RESULT] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Center       0.82      0.81      0.81       832\n",
      "       Donut       0.60      0.78      0.68       146\n",
      "    Edge-Loc       0.79      0.86      0.82      2772\n",
      "   Edge-Ring       0.91      0.74      0.81      1126\n",
      "         Loc       0.73      0.73      0.73      1973\n",
      "   Near-full       0.82      0.98      0.89        95\n",
      "      Random       0.84      0.72      0.78       257\n",
      "     Scratch       0.77      0.69      0.73       693\n",
      "\n",
      "    accuracy                           0.79      7894\n",
      "   macro avg       0.78      0.79      0.78      7894\n",
      "weighted avg       0.79      0.79      0.79      7894\n",
      "\n",
      "[INFO] Model checkpoint saved to model_checkpoints\\baseline\\20250317_012455\\final\\final_model.pth\n",
      "\n",
      "[INFO] Done. Logs are successfully stored in TensorBoard and the Optuna dashboard.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Optionally, set sys.argv to simulate command-line arguments.\n",
    "# This step makes sure that parse_args() in your script picks up the desired configuration.\n",
    "# For example, if you want to use \"config.yaml\", you can do:\n",
    "sys.argv = [\"train_continual.py\", \"--config\", \"config.yaml\"]\n",
    "\n",
    "# Import your training module. Ensure that train_normal.py is in the working directory.\n",
    "import train_normal\n",
    "\n",
    "# Now run the main() function from your module.\n",
    "train_normal.main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded configuration:\n",
      "dataset:\n",
      "  path: D:/Waffer Data/WM811K.pkl\n",
      "experiment:\n",
      "  checkpoint_base_dir: model_checkpoints\n",
      "  continual_learning: true\n",
      "  continual_method: baseline\n",
      "  ewc_lambda: 100.0\n",
      "  final_epochs: 5\n",
      "  final_model_filename: final_model.pth\n",
      "  num_epochs: 5\n",
      "  num_trials: 1\n",
      "  reproducibility: true\n",
      "  save_model: true\n",
      "  seed: 42\n",
      "  suggest:\n",
      "    lr:\n",
      "      high: 1e-3\n",
      "      log: true\n",
      "      low: 1e-5\n",
      "    weight_decay:\n",
      "      high: 1e-2\n",
      "      log: true\n",
      "      low: 1e-6\n",
      "  task_list:\n",
      "  - - 0\n",
      "    - 1\n",
      "  - - 2\n",
      "    - 3\n",
      "  - - 4\n",
      "    - 5\n",
      "  - - 6\n",
      "    - 7\n",
      "  tensorboard_log_dir: Logs\n",
      "logging:\n",
      "  base_log_dir: ./Logs\n",
      "model:\n",
      "  lr: 1e-4\n",
      "  type: resnet50\n",
      "  weight_decay: 1e-4\n",
      "\n",
      "[INFO] Reproducibility enabled. Seed set to 42\n",
      "[INFO] Using device: cuda\n",
      "[INFO] Starting continual learning training pipeline with method: baseline\n",
      "[INFO] Configuration being used:\n",
      "dataset:\n",
      "  path: D:/Waffer Data/WM811K.pkl\n",
      "experiment:\n",
      "  checkpoint_base_dir: model_checkpoints\n",
      "  continual_learning: true\n",
      "  continual_method: baseline\n",
      "  ewc_lambda: 100.0\n",
      "  final_epochs: 5\n",
      "  final_model_filename: final_model.pth\n",
      "  num_epochs: 5\n",
      "  num_trials: 1\n",
      "  reproducibility: true\n",
      "  save_model: true\n",
      "  seed: 42\n",
      "  suggest:\n",
      "    lr:\n",
      "      high: 1e-3\n",
      "      log: true\n",
      "      low: 1e-5\n",
      "    weight_decay:\n",
      "      high: 1e-2\n",
      "      log: true\n",
      "      low: 1e-6\n",
      "  task_list:\n",
      "  - - 0\n",
      "    - 1\n",
      "  - - 2\n",
      "    - 3\n",
      "  - - 4\n",
      "    - 5\n",
      "  - - 6\n",
      "    - 7\n",
      "  tensorboard_log_dir: Logs\n",
      "logging:\n",
      "  base_log_dir: ./Logs\n",
      "model:\n",
      "  lr: 1e-4\n",
      "  type: resnet50\n",
      "  weight_decay: 1e-4\n",
      "\n",
      "\n",
      "[INFO] Starting Task 0 with:\n",
      "        Current task classes: [0, 1]\n",
      "        Global classes so far: [0, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-19 18:18:58,259] A new study created in memory with name: no-name-e72b7848-3d7a-4a13-806d-44c887d9270e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Creating ResNet50 with lr=1.8000102294982517e-05, weight_decay=0.004201805379484738\n",
      "[DEBUG] Creating ResNet50 with lr=1.8000102294982517e-05, weight_decay=0.004201805379484738\n",
      "[DEBUG] Creating ResNet50 with lr=1.8000102294982517e-05, weight_decay=0.004201805379484738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-19 18:22:29,523] Trial 0 finished with value: 0.038732780675802915 and parameters: {'lr': 1.8000102294982517e-05, 'weight_decay': 0.004201805379484738}. Best is trial 0 with value: 0.038732780675802915.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Best hyperparameters for Task 0: lr=1.8000102294982517e-05, weight_decay=0.004201805379484738\n",
      "[DEBUG] Creating ResNet50 with lr=1.8000102294982517e-05, weight_decay=0.004201805379484738\n",
      "[MODEL INFO] Task 0: ResNet has total 23512130 parameters (23512130 trainable).\n",
      "[MODEL INFO] Task 0: Output layer has 2 units.\n",
      "[INFO] TensorBoard log directory for Task 0: ./Logs\\default\\task_0\n",
      "[INFO] Task 0 Epoch 0: Train Loss=0.2850, Val Loss=0.2544\n",
      "[INFO] Task 0 Epoch 0: Train Acc=0.8943, Val Acc=0.8947\n",
      "[INFO] Task 0 Epoch 1: Train Loss=0.0372, Val Loss=0.2281\n",
      "[INFO] Task 0 Epoch 1: Train Acc=0.9928, Val Acc=0.9018\n",
      "[INFO] Task 0 Epoch 2: Train Loss=0.0153, Val Loss=0.2656\n",
      "[INFO] Task 0 Epoch 2: Train Acc=0.9974, Val Acc=0.8978\n",
      "[INFO] Task 0 Epoch 3: Train Loss=0.0074, Val Loss=0.2735\n",
      "[INFO] Task 0 Epoch 3: Train Acc=0.9992, Val Acc=0.8967\n",
      "[INFO] Task 0 Epoch 4: Train Loss=0.0051, Val Loss=0.3708\n",
      "[INFO] Task 0 Epoch 4: Train Acc=1.0000, Val Acc=0.8865\n",
      "[INFO] Finished Task 0\n",
      "[INFO] Model checkpoint saved to model_checkpoints\\baseline\\default\\task0\\model_default.pth\n",
      "[INFO] Evaluating performance for tasks 1 to 1...\n",
      " => Ave accuracy (this task):    88.650\n",
      " => Ave accuracy (tasks so far): 88.650\n",
      "\n",
      "[INFO] Starting Task 1 with:\n",
      "        Current task classes: [2, 3]\n",
      "        Global classes so far: [0, 1, 2, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-19 20:20:04,105] A new study created in memory with name: no-name-a3c112a6-3aff-4d2b-9479-9f9a9883587b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Creating ResNet50 with lr=0.00014438522538815255, weight_decay=0.009409816333024557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-19 20:20:33,514] Trial 0 failed with parameters: {'lr': 0.00014438522538815255, 'weight_decay': 0.009409816333024557} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"d:\\OneDrive\\Documents\\GitHub\\DLProject_1\\ModelTraining\\Wafer-map\\train_continual.py\", line 218, in objective\n",
      "    t_loss, _ = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\OneDrive\\Documents\\GitHub\\DLProject_1\\ModelTraining\\Wafer-map\\utils.py\", line 77, in train_one_epoch\n",
      "    outputs = model(inputs)\n",
      "              ^^^^^^^^^^^^^\n",
      "  File \"d:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\torchvision\\models\\resnet.py\", line 285, in forward\n",
      "    return self._forward_impl(x)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\torchvision\\models\\resnet.py\", line 274, in _forward_impl\n",
      "    x = self.layer2(x)\n",
      "        ^^^^^^^^^^^^^^\n",
      "  File \"d:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py\", line 250, in forward\n",
      "    input = module(input)\n",
      "            ^^^^^^^^^^^^^\n",
      "  File \"d:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\torchvision\\models\\resnet.py\", line 161, in forward\n",
      "    out = self.relu(out)\n",
      "          ^^^^^^^^^^^^^^\n",
      "  File \"d:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 133, in forward\n",
      "    return F.relu(input, inplace=self.inplace)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py\", line 1702, in relu\n",
      "    result = torch.relu_(input)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-03-19 20:20:33,567] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtrain_continual\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Now run the main() function from your module.\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[43mtrain_continual\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\GitHub\\DLProject_1\\ModelTraining\\Wafer-map\\train_continual.py:393\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    391\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Using device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 393\u001b[0m final_model \u001b[38;5;241m=\u001b[39m \u001b[43mcontinual_training_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\GitHub\\DLProject_1\\ModelTraining\\Wafer-map\\train_continual.py:326\u001b[0m, in \u001b[0;36mcontinual_training_pipeline\u001b[1;34m(config, model_factory, device)\u001b[0m\n\u001b[0;32m    324\u001b[0m test_datasets\u001b[38;5;241m.\u001b[39mappend(test_dataset)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;66;03m# Hyperparameter optimization.\u001b[39;00m\n\u001b[1;32m--> 326\u001b[0m best_lr, best_wd \u001b[38;5;241m=\u001b[39m \u001b[43mtune_hyperparameters_for_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexperiment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexperiment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_trials\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_num_classes\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Best hyperparameters for Task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_lr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, weight_decay=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_wd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    333\u001b[0m \u001b[38;5;66;03m# Update or initialize the model.\u001b[39;00m\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\GitHub\\DLProject_1\\ModelTraining\\Wafer-map\\train_continual.py:225\u001b[0m, in \u001b[0;36mtune_hyperparameters_for_task\u001b[1;34m(config, task_dataset, model_factory, device, num_epochs, num_trials, num_classes)\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(fold_losses)\n\u001b[0;32m    224\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 225\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_trials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m best_trial \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m best_trial\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m], best_trial\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    246\u001b[0m ):\n\u001b[1;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\GitHub\\DLProject_1\\ModelTraining\\Wafer-map\\train_continual.py:218\u001b[0m, in \u001b[0;36mtune_hyperparameters_for_task.<locals>.objective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m    216\u001b[0m best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperiment\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m--> 218\u001b[0m     t_loss, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m     v_loss, _ \u001b[38;5;241m=\u001b[39m validate_one_epoch(model, val_loader, criterion, device)\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v_loss \u001b[38;5;241m<\u001b[39m best_val_loss:\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\GitHub\\DLProject_1\\ModelTraining\\Wafer-map\\utils.py:77\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m     75\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     76\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 77\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     79\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\torchvision\\models\\resnet.py:274\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    271\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[0;32m    273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[1;32m--> 274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n\u001b[0;32m    276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\torchvision\\models\\resnet.py:161\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    158\u001b[0m     identity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n\u001b[0;32m    160\u001b[0m out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m identity\n\u001b[1;32m--> 161\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:133\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:1702\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1700\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(relu, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39minplace)\n\u001b[0;32m   1701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m-> 1702\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1704\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "import sys\n",
    "\n",
    "# Optionally, set sys.argv to simulate command-line arguments.\n",
    "# This step makes sure that parse_args() in your script picks up the desired configuration.\n",
    "# For example, if you want to use \"config.yaml\", you can do:\n",
    "sys.argv = [\"train_continual.py\", \"--config\", \"config.yaml\"]\n",
    "\n",
    "# Import your training module. Ensure that train_normal.py is in the working directory.\n",
    "import train_continual\n",
    "\n",
    "# Now run the main() function from your module.\n",
    "train_continual.main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
