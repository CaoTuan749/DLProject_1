{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMARTDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_directory,\n",
    "        models_to_include=None,\n",
    "        days_before_failure=30,\n",
    "        sequence_length=30,\n",
    "        smart_attribute_numbers=[5, 187, 197, 198],\n",
    "        include_raw=True,\n",
    "        include_normalized=True,\n",
    "        enable_bfill=False,\n",
    "        scaler=None,  \n",
    "        model_label_encoder=None,  # To ensure consistent model encoding\n",
    "    ):\n",
    "        self.data_directory = data_directory\n",
    "        self.models_to_include = models_to_include  # List of models to include\n",
    "        self.days_before_failure = days_before_failure\n",
    "        self.sequence_length = sequence_length\n",
    "        self.smart_attribute_numbers = smart_attribute_numbers\n",
    "        self.include_raw = include_raw\n",
    "        self.include_normalized = include_normalized\n",
    "        self.enable_bfill = enable_bfill\n",
    "        self.scaler = scaler  \n",
    "        self.model_label_encoder = model_label_encoder  # Use existing encoder if provided\n",
    "\n",
    "        # Initialize lists for features and labels\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "\n",
    "        # Process the data to populate self.X and self.y\n",
    "        self.process_data()\n",
    "\n",
    "    def process_data(self):\n",
    "        # Generate the list of SMART attribute column names\n",
    "        smart_attributes = []\n",
    "        for num in self.smart_attribute_numbers:\n",
    "            if self.include_raw:\n",
    "                smart_attributes.append(f'smart_{num}_raw')\n",
    "            if self.include_normalized:\n",
    "                smart_attributes.append(f'smart_{num}_normalized')\n",
    "        self.smart_attributes = smart_attributes\n",
    "\n",
    "        # Initialize an empty list to store failure records\n",
    "        failure_records = []\n",
    "\n",
    "        # Get a list of all CSV files in the directory\n",
    "        all_files = glob.glob(os.path.join(self.data_directory, '*.csv'))\n",
    "        all_files.sort()  # Ensure files are processed in order\n",
    "\n",
    "        # Iterate over each file to collect failure records\n",
    "        for filename in all_files:\n",
    "            df = pd.read_csv(filename, usecols=['date', 'serial_number', 'failure'])\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            failures = df[df['failure'] == 1]\n",
    "            if not failures.empty:\n",
    "                failure_records.append(failures)\n",
    "\n",
    "        # Concatenate all failure records into a single DataFrame\n",
    "        failure_data = pd.concat(failure_records, ignore_index=True)\n",
    "\n",
    "        # Get the list of failed drives and their failure dates\n",
    "        failed_drives_info = failure_data.groupby('serial_number')['date'].max().reset_index()\n",
    "        failed_drives_info.rename(columns={'date': 'failure_date'}, inplace=True)\n",
    "\n",
    "        # Dictionary to hold data for each failed drive\n",
    "        failed_drives_data = defaultdict(list)\n",
    "\n",
    "        # Convert failed_drives_info to a dictionary\n",
    "        failure_date_dict = failed_drives_info.set_index('serial_number')['failure_date'].to_dict()\n",
    "\n",
    "        # Columns to read from each CSV file\n",
    "        columns_to_read = ['date', 'serial_number', 'model'] + self.smart_attributes + ['failure']\n",
    "\n",
    "        # Iterate over each file to collect data for failed drives\n",
    "        for filename in all_files:\n",
    "            df = pd.read_csv(filename, usecols=columns_to_read)\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            df_failed = df[df['serial_number'].isin(failure_date_dict.keys())]\n",
    "            if df_failed.empty:\n",
    "                continue  # Skip if no failed drives are present in this file\n",
    "            for serial_number, group in df_failed.groupby('serial_number'):\n",
    "                failed_drives_data[serial_number].append(group)\n",
    "\n",
    "        # Initialize an empty list to store filtered data\n",
    "        filtered_data_list = []\n",
    "\n",
    "        for serial_number, data_list in failed_drives_data.items():\n",
    "            drive_data = pd.concat(data_list, ignore_index=True)\n",
    "            failure_date = failure_date_dict[serial_number].normalize()\n",
    "            drive_data['date'] = drive_data['date'].dt.normalize()\n",
    "            start_date = failure_date - pd.Timedelta(days=self.days_before_failure)\n",
    "            drive_data = drive_data[(drive_data['date'] >= start_date) & (drive_data['date'] <= failure_date)]\n",
    "            drive_data['days_until_failure'] = (failure_date - drive_data['date']).dt.days\n",
    "            drive_data = drive_data[\n",
    "                (drive_data['days_until_failure'] >= 0) & (drive_data['days_until_failure'] <= self.days_before_failure)\n",
    "            ]\n",
    "            filtered_data_list.append(drive_data)\n",
    "\n",
    "        # Concatenate all filtered data\n",
    "        filtered_data = pd.concat(filtered_data_list, ignore_index=True)\n",
    "\n",
    "        # Encode the model types\n",
    "        if self.model_label_encoder is None:\n",
    "            le_model = LabelEncoder()\n",
    "            filtered_data['model_encoded'] = le_model.fit_transform(filtered_data['model'])\n",
    "            self.model_label_encoder = le_model\n",
    "        else:\n",
    "            # Use existing label encoder\n",
    "            filtered_data['model_encoded'] = self.model_label_encoder.transform(filtered_data['model'])\n",
    "\n",
    "        self.model_mapping = dict(zip(self.model_label_encoder.classes_, self.model_label_encoder.transform(self.model_label_encoder.classes_)))\n",
    "\n",
    "        # If models_to_include is provided, filter the data\n",
    "        if self.models_to_include is not None:\n",
    "            filtered_data = filtered_data[filtered_data['model_encoded'].isin(self.models_to_include)].reset_index(drop=True)\n",
    "\n",
    "        # Initialize lists for features and labels\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        # Group data by serial_number\n",
    "        grouped = filtered_data.groupby('serial_number')\n",
    "\n",
    "        for name, group in grouped:\n",
    "            group = group.sort_values(by='date').reset_index(drop=True)\n",
    "            date_to_data = group.set_index('date').to_dict('index')\n",
    "            dates = group['date'].unique()\n",
    "            failure_date = failure_date_dict[name].normalize()\n",
    "            for current_date in dates:\n",
    "                days_until_failure = (failure_date - current_date).days\n",
    "                if days_until_failure < 0 or days_until_failure > self.days_before_failure:\n",
    "                    continue\n",
    "                sequence_dates = [\n",
    "                    current_date - pd.Timedelta(days=i) for i in range(self.sequence_length - 1, -1, -1)\n",
    "                ]\n",
    "                sequence_records = []\n",
    "                for seq_date in sequence_dates:\n",
    "                    if seq_date in date_to_data:\n",
    "                        seq_record = date_to_data[seq_date]\n",
    "                        smart_values = {attr: seq_record.get(attr, np.nan) for attr in self.smart_attributes}\n",
    "                    else:\n",
    "                        smart_values = {attr: np.nan for attr in self.smart_attributes}\n",
    "                    sequence_records.append(smart_values)\n",
    "                sequence_df = pd.DataFrame(sequence_records)\n",
    "                missing_count = sequence_df[self.smart_attributes].isna().sum().sum()\n",
    "                total_values = self.sequence_length * len(self.smart_attributes)\n",
    "                if missing_count > total_values / 2:\n",
    "                    continue  # Discard this data point\n",
    "                sequence_df[self.smart_attributes] = sequence_df[self.smart_attributes].ffill()\n",
    "                if self.enable_bfill:\n",
    "                    sequence_df[self.smart_attributes] = sequence_df[self.smart_attributes].bfill()\n",
    "                sequence_df[self.smart_attributes] = sequence_df[self.smart_attributes].fillna(0)\n",
    "                sequence_data = sequence_df[self.smart_attributes].values.flatten()\n",
    "                model_encoded = group['model_encoded'].iloc[0]\n",
    "                features = [model_encoded] + sequence_data.tolist()\n",
    "                X.append(features)\n",
    "                y.append(days_until_failure)\n",
    "\n",
    "        # Convert features and labels to NumPy arrays\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "\n",
    "        # Normalize SMART attributes (exclude model_encoded)\n",
    "        if self.scaler is None:\n",
    "            scaler = StandardScaler()\n",
    "            X[:, 1:] = scaler.fit_transform(X[:, 1:])\n",
    "            self.scaler = scaler  # Save scaler for future use\n",
    "        else:\n",
    "            X[:, 1:] = self.scaler.transform(X[:, 1:])  # Use existing scaler\n",
    "\n",
    "        # Save features and labels\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = self.X[idx]\n",
    "        label = self.y[idx]\n",
    "        features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32).unsqueeze(0)  # For regression\n",
    "        return features_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sample counts:\n",
      "    model_encoded               model_name  sample_count\n",
      "0               0     HGST HMS5C4040ALE640            12\n",
      "1               1     HGST HMS5C4040BLE640            60\n",
      "2               2     HGST HUH721212ALE600            24\n",
      "3               3     HGST HUH721212ALE604           516\n",
      "4               4     HGST HUH721212ALN604           777\n",
      "5               5     HGST HUH728080ALE600            24\n",
      "6               6            ST10000NM0086           110\n",
      "7               7            ST12000NM0007           470\n",
      "8               8            ST12000NM0008          1830\n",
      "9              10            ST12000NM001G           524\n",
      "10             11            ST14000NM000J            13\n",
      "11             12            ST14000NM001G           500\n",
      "12             13            ST14000NM0138           229\n",
      "13             14            ST16000NM001G           555\n",
      "14             15            ST18000NM000J            17\n",
      "15             16              ST4000DM000           657\n",
      "16             17            ST500LM012 HN            60\n",
      "17             18               ST500LM030            17\n",
      "18             19              ST8000DM002           452\n",
      "19             20             ST8000NM0055           976\n",
      "20             23          TOSHIBA HDWF180            12\n",
      "21             24      TOSHIBA MG07ACA14TA           837\n",
      "22             25     TOSHIBA MG07ACA14TEY             3\n",
      "23             26      TOSHIBA MG08ACA16TA           672\n",
      "24             27      TOSHIBA MG08ACA16TE           108\n",
      "25             28     TOSHIBA MG08ACA16TEY            12\n",
      "26             29       TOSHIBA MQ01ABF050            68\n",
      "27             30      TOSHIBA MQ01ABF050M            94\n",
      "28             31  WD Blue SA510 2.5 250GB             6\n",
      "29             32      WDC WUH721414ALE6L4           110\n",
      "30             33      WDC WUH721816ALE6L0            84\n",
      "31             34      WDC WUH721816ALE6L4            91\n",
      "32             35      WDC WUH722222ALE6L4            31\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 48\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Instead of stratifying by sample_count, we stratify by model_encoded.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Keep only models that have at least 2 samples\u001b[39;00m\n\u001b[0;32m     46\u001b[0m valid_models \u001b[38;5;241m=\u001b[39m model_info_df[model_info_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_count\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m---> 48\u001b[0m train_models_encoded, test_models_encoded \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_models\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_encoded\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_models\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_encoded\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain models (encoded): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_models_encoded\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest models (encoded): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_models_encoded\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2799\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2795\u001b[0m         CVClass \u001b[38;5;241m=\u001b[39m ShuffleSplit\n\u001b[0;32m   2797\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m-> 2799\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstratify\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2801\u001b[0m train, test \u001b[38;5;241m=\u001b[39m ensure_common_namespace_device(arrays[\u001b[38;5;241m0\u001b[39m], train, test)\n\u001b[0;32m   2803\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   2804\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[0;32m   2805\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[0;32m   2806\u001b[0m     )\n\u001b[0;32m   2807\u001b[0m )\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:1841\u001b[0m, in \u001b[0;36mBaseShuffleSplit.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m   1811\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[0;32m   1812\u001b[0m \n\u001b[0;32m   1813\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1838\u001b[0m \u001b[38;5;124;03mto an integer.\u001b[39;00m\n\u001b[0;32m   1839\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1840\u001b[0m X, y, groups \u001b[38;5;241m=\u001b[39m indexable(X, y, groups)\n\u001b[1;32m-> 1841\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1842\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\n",
      "File \u001b[1;32md:\\OneDrive\\Documents\\GitHub\\DLProject_1\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2245\u001b[0m, in \u001b[0;36mStratifiedShuffleSplit._iter_indices\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m   2243\u001b[0m class_counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbincount(y_indices)\n\u001b[0;32m   2244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmin(class_counts) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m-> 2245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2246\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe least populated class in y has only 1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2247\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m member, which is too few. The minimum\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2248\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m number of groups for any class cannot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2249\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be less than 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2250\u001b[0m     )\n\u001b[0;32m   2252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m<\u001b[39m n_classes:\n\u001b[0;32m   2253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2254\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe train_size = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m should be greater or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2255\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mequal to the number of classes = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (n_train, n_classes)\n\u001b[0;32m   2256\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "data_directory = 'D:/Backblaze_Data/data_Q2_2024/Training-Q1-data/'\n",
    "days_before_failure = 30\n",
    "sequence_length = 30\n",
    "smart_attribute_numbers = [5, 187, 197, 198]\n",
    "include_raw = True\n",
    "include_normalized = True\n",
    "enable_bfill = False\n",
    "\n",
    "# First, create a full dataset to get models and their sample counts\n",
    "full_dataset = SMARTDataset(\n",
    "    data_directory=data_directory,\n",
    "    days_before_failure=days_before_failure,\n",
    "    sequence_length=sequence_length,\n",
    "    smart_attribute_numbers=smart_attribute_numbers,\n",
    "    include_raw=include_raw,\n",
    "    include_normalized=include_normalized,\n",
    "    enable_bfill=enable_bfill,\n",
    "    scaler=None,\n",
    "    model_label_encoder=None,  \n",
    ")\n",
    "\n",
    "# Extract models and their sample counts\n",
    "model_encoded_list = full_dataset.X[:, 0].astype(int)\n",
    "model_counts = pd.Series(model_encoded_list).value_counts().sort_index()\n",
    "model_indices = model_counts.index.tolist()\n",
    "model_sample_counts = model_counts.values.tolist()\n",
    "\n",
    "# Map encoded model indices back to model names\n",
    "model_names = [full_dataset.model_label_encoder.inverse_transform([idx])[0] for idx in model_indices]\n",
    "\n",
    "# Create a DataFrame for models and sample counts\n",
    "model_info_df = pd.DataFrame({\n",
    "    'model_encoded': model_indices,\n",
    "    'model_name': model_names,\n",
    "    'sample_count': model_sample_counts,\n",
    "})\n",
    "\n",
    "print(\"Model sample counts:\")\n",
    "print(model_info_df)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Instead of stratifying by sample_count, we stratify by model_encoded.\n",
    "\n",
    "train_models_encoded, test_models_encoded = train_test_split(\n",
    "    model_info_df['model_encoded'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Train models (encoded): {train_models_encoded.tolist()}\")\n",
    "print(f\"Test models (encoded): {test_models_encoded.tolist()}\")\n",
    "\n",
    "# Create training dataset with models in train_models_encoded\n",
    "train_dataset = SMARTDataset(\n",
    "    data_directory=data_directory,\n",
    "    models_to_include=train_models_encoded.tolist(),\n",
    "    days_before_failure=days_before_failure,\n",
    "    sequence_length=sequence_length,\n",
    "    smart_attribute_numbers=smart_attribute_numbers,\n",
    "    include_raw=include_raw,\n",
    "    include_normalized=include_normalized,\n",
    "    enable_bfill=enable_bfill,\n",
    "    scaler=None,  # Scaler will be created using training data\n",
    "    model_label_encoder=full_dataset.model_label_encoder,  # Use the same encoder\n",
    ")\n",
    "\n",
    "# Create test dataset with models in test_models_encoded, using the same scaler as training data\n",
    "test_dataset = SMARTDataset(\n",
    "    data_directory=data_directory,\n",
    "    models_to_include=test_models_encoded.tolist(),\n",
    "    days_before_failure=days_before_failure,\n",
    "    sequence_length=sequence_length,\n",
    "    smart_attribute_numbers=smart_attribute_numbers,\n",
    "    include_raw=include_raw,\n",
    "    include_normalized=include_normalized,\n",
    "    enable_bfill=enable_bfill,\n",
    "    scaler=train_dataset.scaler,  # Use scaler from training data\n",
    "    model_label_encoder=full_dataset.model_label_encoder,  # Use the same encoder\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64  \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"Data loaders created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
