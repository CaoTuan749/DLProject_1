{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMARTDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_directory,\n",
    "        models_to_include=None,\n",
    "        days_before_failure=30,\n",
    "        sequence_length=30,\n",
    "        smart_attribute_numbers=[5, 187, 197, 198],\n",
    "        include_raw=True,\n",
    "        include_normalized=True,\n",
    "        enable_bfill=False,\n",
    "        scaler=None,  \n",
    "        model_label_encoder=None,  # To ensure consistent model encoding\n",
    "    ):\n",
    "        self.data_directory = data_directory\n",
    "        self.models_to_include = models_to_include  # List of models to include\n",
    "        self.days_before_failure = days_before_failure\n",
    "        self.sequence_length = sequence_length\n",
    "        self.smart_attribute_numbers = smart_attribute_numbers\n",
    "        self.include_raw = include_raw\n",
    "        self.include_normalized = include_normalized\n",
    "        self.enable_bfill = enable_bfill\n",
    "        self.scaler = scaler  \n",
    "        self.model_label_encoder = model_label_encoder  # Use existing encoder if provided\n",
    "\n",
    "        # Initialize lists for features and labels\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "\n",
    "        # Process the data to populate self.X and self.y\n",
    "        self.process_data()\n",
    "\n",
    "    def process_data(self):\n",
    "        # Generate the list of SMART attribute column names\n",
    "        smart_attributes = []\n",
    "        for num in self.smart_attribute_numbers:\n",
    "            if self.include_raw:\n",
    "                smart_attributes.append(f'smart_{num}_raw')\n",
    "            if self.include_normalized:\n",
    "                smart_attributes.append(f'smart_{num}_normalized')\n",
    "        self.smart_attributes = smart_attributes\n",
    "\n",
    "        # Initialize an empty list to store failure records\n",
    "        failure_records = []\n",
    "\n",
    "        # Get a list of all CSV files in the directory\n",
    "        all_files = glob.glob(os.path.join(self.data_directory, '*.csv'))\n",
    "        all_files.sort()  # Ensure files are processed in order\n",
    "\n",
    "        # Iterate over each file to collect failure records\n",
    "        for filename in all_files:\n",
    "            df = pd.read_csv(filename, usecols=['date', 'serial_number', 'failure'])\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            failures = df[df['failure'] == 1]\n",
    "            if not failures.empty:\n",
    "                failure_records.append(failures)\n",
    "\n",
    "        # Concatenate all failure records into a single DataFrame\n",
    "        failure_data = pd.concat(failure_records, ignore_index=True)\n",
    "\n",
    "        # Get the list of failed drives and their failure dates\n",
    "        failed_drives_info = failure_data.groupby('serial_number')['date'].max().reset_index()\n",
    "        failed_drives_info.rename(columns={'date': 'failure_date'}, inplace=True)\n",
    "\n",
    "        # Dictionary to hold data for each failed drive\n",
    "        failed_drives_data = defaultdict(list)\n",
    "\n",
    "        # Convert failed_drives_info to a dictionary\n",
    "        failure_date_dict = failed_drives_info.set_index('serial_number')['failure_date'].to_dict()\n",
    "\n",
    "        # Columns to read from each CSV file\n",
    "        columns_to_read = ['date', 'serial_number', 'model'] + self.smart_attributes + ['failure']\n",
    "\n",
    "        # Iterate over each file to collect data for failed drives\n",
    "        for filename in all_files:\n",
    "            df = pd.read_csv(filename, usecols=columns_to_read)\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            df_failed = df[df['serial_number'].isin(failure_date_dict.keys())]\n",
    "            if df_failed.empty:\n",
    "                continue  # Skip if no failed drives are present in this file\n",
    "            for serial_number, group in df_failed.groupby('serial_number'):\n",
    "                failed_drives_data[serial_number].append(group)\n",
    "\n",
    "        # Initialize an empty list to store filtered data\n",
    "        filtered_data_list = []\n",
    "\n",
    "        for serial_number, data_list in failed_drives_data.items():\n",
    "            drive_data = pd.concat(data_list, ignore_index=True)\n",
    "            failure_date = failure_date_dict[serial_number].normalize()\n",
    "            drive_data['date'] = drive_data['date'].dt.normalize()\n",
    "            start_date = failure_date - pd.Timedelta(days=self.days_before_failure)\n",
    "            drive_data = drive_data[(drive_data['date'] >= start_date) & (drive_data['date'] <= failure_date)]\n",
    "            drive_data['days_until_failure'] = (failure_date - drive_data['date']).dt.days\n",
    "            drive_data = drive_data[\n",
    "                (drive_data['days_until_failure'] >= 0) & (drive_data['days_until_failure'] <= self.days_before_failure)\n",
    "            ]\n",
    "            filtered_data_list.append(drive_data)\n",
    "\n",
    "        # Concatenate all filtered data\n",
    "        filtered_data = pd.concat(filtered_data_list, ignore_index=True)\n",
    "\n",
    "        # Encode the model types\n",
    "        if self.model_label_encoder is None:\n",
    "            le_model = LabelEncoder()\n",
    "            filtered_data['model_encoded'] = le_model.fit_transform(filtered_data['model'])\n",
    "            self.model_label_encoder = le_model\n",
    "        else:\n",
    "            # Use existing label encoder\n",
    "            filtered_data['model_encoded'] = self.model_label_encoder.transform(filtered_data['model'])\n",
    "\n",
    "        self.model_mapping = dict(zip(self.model_label_encoder.classes_, self.model_label_encoder.transform(self.model_label_encoder.classes_)))\n",
    "\n",
    "        # If models_to_include is provided, filter the data\n",
    "        if self.models_to_include is not None:\n",
    "            filtered_data = filtered_data[filtered_data['model_encoded'].isin(self.models_to_include)].reset_index(drop=True)\n",
    "\n",
    "        # Initialize lists for features and labels\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        # Group data by serial_number\n",
    "        grouped = filtered_data.groupby('serial_number')\n",
    "\n",
    "        for name, group in grouped:\n",
    "            group = group.sort_values(by='date').reset_index(drop=True)\n",
    "            date_to_data = group.set_index('date').to_dict('index')\n",
    "            dates = group['date'].unique()\n",
    "            failure_date = failure_date_dict[name].normalize()\n",
    "            for current_date in dates:\n",
    "                days_until_failure = (failure_date - current_date).days\n",
    "                if days_until_failure < 0 or days_until_failure > self.days_before_failure:\n",
    "                    continue\n",
    "                sequence_dates = [\n",
    "                    current_date - pd.Timedelta(days=i) for i in range(self.sequence_length - 1, -1, -1)\n",
    "                ]\n",
    "                sequence_records = []\n",
    "                for seq_date in sequence_dates:\n",
    "                    if seq_date in date_to_data:\n",
    "                        seq_record = date_to_data[seq_date]\n",
    "                        smart_values = {attr: seq_record.get(attr, np.nan) for attr in self.smart_attributes}\n",
    "                    else:\n",
    "                        smart_values = {attr: np.nan for attr in self.smart_attributes}\n",
    "                    sequence_records.append(smart_values)\n",
    "                sequence_df = pd.DataFrame(sequence_records)\n",
    "                missing_count = sequence_df[self.smart_attributes].isna().sum().sum()\n",
    "                total_values = self.sequence_length * len(self.smart_attributes)\n",
    "                if missing_count > total_values / 2:\n",
    "                    continue  # Discard this data point\n",
    "                sequence_df[self.smart_attributes] = sequence_df[self.smart_attributes].ffill()\n",
    "                if self.enable_bfill:\n",
    "                    sequence_df[self.smart_attributes] = sequence_df[self.smart_attributes].bfill()\n",
    "                sequence_df[self.smart_attributes] = sequence_df[self.smart_attributes].fillna(0)\n",
    "                sequence_data = sequence_df[self.smart_attributes].values.flatten()\n",
    "                model_encoded = group['model_encoded'].iloc[0]\n",
    "                features = [model_encoded] + sequence_data.tolist()\n",
    "                X.append(features)\n",
    "                y.append(days_until_failure)\n",
    "\n",
    "        # Convert features and labels to NumPy arrays\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "\n",
    "        # Normalize SMART attributes (exclude model_encoded)\n",
    "        if self.scaler is None:\n",
    "            scaler = StandardScaler()\n",
    "            X[:, 1:] = scaler.fit_transform(X[:, 1:])\n",
    "            self.scaler = scaler  # Save scaler for future use\n",
    "        else:\n",
    "            X[:, 1:] = self.scaler.transform(X[:, 1:])  # Use existing scaler\n",
    "\n",
    "        # Save features and labels\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = self.X[idx]\n",
    "        label = self.y[idx]\n",
    "        features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32).unsqueeze(0)  # For regression\n",
    "        return features_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m enable_bfill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# First, create a full dataset to get models and their sample counts\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m full_dataset \u001b[38;5;241m=\u001b[39m SMARTDataset(\n\u001b[1;32m     13\u001b[0m     data_directory\u001b[38;5;241m=\u001b[39mdata_directory,\n\u001b[1;32m     14\u001b[0m     days_before_failure\u001b[38;5;241m=\u001b[39mdays_before_failure,\n\u001b[1;32m     15\u001b[0m     sequence_length\u001b[38;5;241m=\u001b[39msequence_length,\n\u001b[1;32m     16\u001b[0m     smart_attribute_numbers\u001b[38;5;241m=\u001b[39msmart_attribute_numbers,\n\u001b[1;32m     17\u001b[0m     include_raw\u001b[38;5;241m=\u001b[39minclude_raw,\n\u001b[1;32m     18\u001b[0m     include_normalized\u001b[38;5;241m=\u001b[39minclude_normalized,\n\u001b[1;32m     19\u001b[0m     enable_bfill\u001b[38;5;241m=\u001b[39menable_bfill,\n\u001b[1;32m     20\u001b[0m     scaler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m     model_label_encoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Extract models and their sample counts\u001b[39;00m\n\u001b[1;32m     25\u001b[0m model_encoded_list \u001b[38;5;241m=\u001b[39m full_dataset\u001b[38;5;241m.\u001b[39mX[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 31\u001b[0m, in \u001b[0;36mSMARTDataset.__init__\u001b[0;34m(self, data_directory, models_to_include, days_before_failure, sequence_length, smart_attribute_numbers, include_raw, include_normalized, enable_bfill, scaler, model_label_encoder)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Process the data to populate self.X and self.y\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_data()\n",
      "Cell \u001b[0;32mIn[4], line 59\u001b[0m, in \u001b[0;36mSMARTDataset.process_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m         failure_records\u001b[38;5;241m.\u001b[39mappend(failures)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Concatenate all failure records into a single DataFrame\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m failure_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(failure_records, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Get the list of failed drives and their failure dates\u001b[39;00m\n\u001b[1;32m     62\u001b[0m failed_drives_info \u001b[38;5;241m=\u001b[39m failure_data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserial_number\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "File \u001b[0;32m~/tmp/miniconda3/envs/MaProject/lib/python3.12/site-packages/pandas/core/reshape/concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[1;32m    383\u001b[0m     objs,\n\u001b[1;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m    385\u001b[0m     ignore_index\u001b[38;5;241m=\u001b[39mignore_index,\n\u001b[1;32m    386\u001b[0m     join\u001b[38;5;241m=\u001b[39mjoin,\n\u001b[1;32m    387\u001b[0m     keys\u001b[38;5;241m=\u001b[39mkeys,\n\u001b[1;32m    388\u001b[0m     levels\u001b[38;5;241m=\u001b[39mlevels,\n\u001b[1;32m    389\u001b[0m     names\u001b[38;5;241m=\u001b[39mnames,\n\u001b[1;32m    390\u001b[0m     verify_integrity\u001b[38;5;241m=\u001b[39mverify_integrity,\n\u001b[1;32m    391\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    393\u001b[0m )\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/tmp/miniconda3/envs/MaProject/lib/python3.12/site-packages/pandas/core/reshape/concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[0;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clean_keys_and_objs(objs, keys)\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[1;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[0;32m~/tmp/miniconda3/envs/MaProject/lib/python3.12/site-packages/pandas/core/reshape/concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "# data_directory = 'D:/Backblaze_Data/data_Q2_2024/Training-Q1-data/'\n",
    "data_directory = '~/tmp/Dataset/Harddrive-dataset/data-Q1-2024/'\n",
    "days_before_failure = 30\n",
    "sequence_length = 30\n",
    "smart_attribute_numbers = [5, 187, 197, 198]\n",
    "include_raw = True\n",
    "include_normalized = True\n",
    "enable_bfill = False\n",
    "\n",
    "# First, create a full dataset to get models and their sample counts\n",
    "full_dataset = SMARTDataset(\n",
    "    data_directory=data_directory,\n",
    "    days_before_failure=days_before_failure,\n",
    "    sequence_length=sequence_length,\n",
    "    smart_attribute_numbers=smart_attribute_numbers,\n",
    "    include_raw=include_raw,\n",
    "    include_normalized=include_normalized,\n",
    "    enable_bfill=enable_bfill,\n",
    "    scaler=None,\n",
    "    model_label_encoder=None,  \n",
    ")\n",
    "\n",
    "# Extract models and their sample counts\n",
    "model_encoded_list = full_dataset.X[:, 0].astype(int)\n",
    "model_counts = pd.Series(model_encoded_list).value_counts().sort_index()\n",
    "model_indices = model_counts.index.tolist()\n",
    "model_sample_counts = model_counts.values.tolist()\n",
    "\n",
    "# Map encoded model indices back to model names\n",
    "model_names = [full_dataset.model_label_encoder.inverse_transform([idx])[0] for idx in model_indices]\n",
    "\n",
    "# Create a DataFrame for models and sample counts\n",
    "model_info_df = pd.DataFrame({\n",
    "    'model_encoded': model_indices,\n",
    "    'model_name': model_names,\n",
    "    'sample_count': model_sample_counts,\n",
    "})\n",
    "\n",
    "print(\"Model sample counts:\")\n",
    "print(model_info_df)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Instead of stratifying by sample_count, we stratify by model_encoded.\n",
    "\n",
    "train_models_encoded, test_models_encoded = train_test_split(\n",
    "    model_info_df['model_encoded'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Train models (encoded): {train_models_encoded.tolist()}\")\n",
    "print(f\"Test models (encoded): {test_models_encoded.tolist()}\")\n",
    "\n",
    "# Create training dataset with models in train_models_encoded\n",
    "train_dataset = SMARTDataset(\n",
    "    data_directory=data_directory,\n",
    "    models_to_include=train_models_encoded.tolist(),\n",
    "    days_before_failure=days_before_failure,\n",
    "    sequence_length=sequence_length,\n",
    "    smart_attribute_numbers=smart_attribute_numbers,\n",
    "    include_raw=include_raw,\n",
    "    include_normalized=include_normalized,\n",
    "    enable_bfill=enable_bfill,\n",
    "    scaler=None,  # Scaler will be created using training data\n",
    "    model_label_encoder=full_dataset.model_label_encoder,  # Use the same encoder\n",
    ")\n",
    "\n",
    "# Create test dataset with models in test_models_encoded, using the same scaler as training data\n",
    "test_dataset = SMARTDataset(\n",
    "    data_directory=data_directory,\n",
    "    models_to_include=test_models_encoded.tolist(),\n",
    "    days_before_failure=days_before_failure,\n",
    "    sequence_length=sequence_length,\n",
    "    smart_attribute_numbers=smart_attribute_numbers,\n",
    "    include_raw=include_raw,\n",
    "    include_normalized=include_normalized,\n",
    "    enable_bfill=enable_bfill,\n",
    "    scaler=train_dataset.scaler,  # Use scaler from training data\n",
    "    model_label_encoder=full_dataset.model_label_encoder,  # Use the same encoder\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64  \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"Data loaders created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MaProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
